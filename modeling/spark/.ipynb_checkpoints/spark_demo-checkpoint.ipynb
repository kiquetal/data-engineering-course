{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Spark 4.0.0 Demo Notebook\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates using PySpark with the latest version of Apache Spark.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Setup and Configuration\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, let's import the necessary libraries and configure Spark:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import tempfile\\n\",\n",
    "    \"from pyspark.sql import SparkSession\\n\",\n",
    "    \"from pyspark.sql.functions import col, lit, expr\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Local Testing Configuration\\n\",\n",
    "    \"\\n\",\n",
    "    \"Depending on your environment, you might need to handle permissions differently:\\n\",\n",
    "    \"1. If you've changed ownership of the artifacts folder (`chmod -R 777 artifacts/`), you can use the default setup.\\n\",\n",
    "    \"2. If you're having permission issues, uncomment and use the temporary directory configuration.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Uncomment these lines if you have permission issues:\\n\",\n",
    "    \"# tmp_dir = tempfile.mkdtemp()\\n\",\n",
    "    \"# os.environ['SPARK_LOCAL_DIRS'] = tmp_dir\\n\",\n",
    "    \"# print(f\\\"Using temporary directory: {tmp_dir}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Initialize Spark Session\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now we'll create a SparkSession with the appropriate configurations:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Create a SparkSession\\n\",\n",
    "    \"spark = (SparkSession.builder\\n\",\n",
    "    \"         .appName(\\\"SparkNotebookDemo\\\")\\n\",\n",
    "    \"         .config(\\\"spark.sql.shuffle.partitions\\\", \\\"2\\\")  # Performance config for small datasets\\n\",\n",
    "    \"         .config(\\\"spark.executor.memory\\\", \\\"1g\\\")\\n\",\n",
    "    \"         .config(\\\"spark.driver.memory\\\", \\\"1g\\\")\\n\",\n",
    "    \"         # Fix for security manager issue in Spark 4.0.0\\n\",\n",
    "    \"         .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Djava.security.manager=allow\\\")\\n\",\n",
    "    \"         .config(\\\"spark.executor.extraJavaOptions\\\", \\\"-Djava.security.manager=allow\\\")\\n\",\n",
    "    \"         \\n\",\n",
    "    \"         # Uncomment these lines if using temporary directory\\n\",\n",
    "    \"         # .config(\\\"spark.local.dir\\\", tmp_dir)\\n\",\n",
    "    \"         # .config(\\\"spark.worker.dir\\\", tmp_dir)\\n\",\n",
    "    \"         # .config(\\\"spark.sql.warehouse.dir\\\", f\\\"{tmp_dir}/spark-warehouse\\\")\\n\",\n",
    "    \"         \\n\",\n",
    "    \"         .master(\\\"local[*]\\\")  # Explicitly set to local mode\\n\",\n",
    "    \"         .getOrCreate())\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Spark version: {spark.version}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Create Sample Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's create a simple dataset to work with:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Sample data\\n\",\n",
    "    \"data = [\\n\",\n",
    "    \"    (\\\"Alice\\\", 34, \\\"Data Engineer\\\"),\\n\",\n",
    "    \"    (\\\"Bob\\\", 45, \\\"Data Scientist\\\"),\\n\",\n",
    "    \"    (\\\"Charlie\\\", 29, \\\"ML Engineer\\\"),\\n\",\n",
    "    \"    (\\\"Diana\\\", 37, \\\"Data Analyst\\\"),\\n\",\n",
    "    \"    (\\\"Eve\\\", 31, \\\"Software Engineer\\\")\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create DataFrame\\n\",\n",
    "    \"columns = [\\\"name\\\", \\\"age\\\", \\\"job_title\\\"]\\n\",\n",
    "    \"df = spark.createDataFrame(data, columns)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Sample DataFrame:\\\")\\n\",\n",
    "    \"df.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Basic Transformations\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's perform some basic transformations on our DataFrame:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Select, filter, add column, and sort\\n\",\n",
    "    \"(df.select(\\\"name\\\", \\\"job_title\\\", col(\\\"age\\\").cast(\\\"int\\\"))\\n\",\n",
    "    \"   .filter(col(\\\"age\\\") > 30)\\n\",\n",
    "    \"   .withColumn(\\\"department\\\", lit(\\\"Data\\\"))\\n\",\n",
    "    \"   .orderBy(\\\"age\\\")\\n\",\n",
    "    \"   .show())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## SQL Queries\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's use Spark SQL to query our data:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Register DataFrame as a temporary view\\n\",\n",
    "    \"df.createOrReplaceTempView(\\\"employees\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run SQL query\\n\",\n",
    "    \"spark.sql(\\\"\\\"\\\"\\n\",\n",
    "    \"    SELECT name, job_title, age\\n\",\n",
    "    \"    FROM employees\\n\",\n",
    "    \"    WHERE age > 35\\n\",\n",
    "    \"    ORDER BY age DESC\\n\",\n",
    "    \"\\\"\\\"\\\").show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Using Expressions\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's use Spark expressions for more complex transformations:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Use expressions for categorization\\n\",\n",
    "    \"(df.withColumn(\\\"experience_category\\\", \\n\",\n",
    "    \"              expr(\\\"CASE WHEN age < 30 THEN 'Junior' \\\" +\\n\",\n",
    "    \"                   \\\"WHEN age <= 40 THEN 'Mid-level' \\\" +\\n\",\n",
    "    \"                   \\\"ELSE 'Senior' END\\\"))\\n\",\n",
    "    \"   .show())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Advanced Example: Data Aggregation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's extend the examples with some aggregation operations:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from pyspark.sql.functions import avg, count, max, min\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add more sample data for meaningful aggregation\\n\",\n",
    "    \"more_data = data + [\\n\",\n",
    "    \"    (\\\"Frank\\\", 42, \\\"Data Scientist\\\"),\\n\",\n",
    "    \"    (\\\"Grace\\\", 38, \\\"ML Engineer\\\"),\\n\",\n",
    "    \"    (\\\"Hannah\\\", 27, \\\"Data Engineer\\\"),\\n\",\n",
    "    \"    (\\\"Ian\\\", 51, \\\"Data Scientist\\\")\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a new DataFrame with more data\\n\",\n",
    "    \"df_extended = spark.createDataFrame(more_data, columns)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Aggregation by job title\\n\",\n",
    "    \"job_stats = df_extended.groupBy(\\\"job_title\\\").agg(\\n\",\n",
    "    \"    count(\\\"*\\\").alias(\\\"count\\\"),\\n\",\n",
    "    \"    avg(\\\"age\\\").alias(\\\"avg_age\\\"),\\n\",\n",
    "    \"    min(\\\"age\\\").alias(\\\"min_age\\\"),\\n\",\n",
    "    \"    max(\\\"age\\\").alias(\\\"max_age\\\")\\n\",\n",
    "    \").orderBy(\\\"job_title\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Job Title Statistics:\\\")\\n\",\n",
    "    \"job_stats.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Cleanup\\n\",\n",
    "    \"\\n\",\n",
    "    \"Always stop the SparkSession when finished:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Stop the SparkSession\\n\",\n",
    "    \"spark.stop()\\n\",\n",
    "    \"print(\\\"SparkSession closed successfully\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# If you used a temporary directory, you can clean it up\\n\",\n",
    "    \"# import shutil\\n\",\n",
    "    \"# shutil.rmtree(tmp_dir)\\n\",\n",
    "    \"# print(f\\\"Cleaned up temporary directory: {tmp_dir}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n",
    "\n"
   ],
   "id": "5ab6ab154dd1a8d4"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
